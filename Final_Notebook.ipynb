{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde8f59f",
   "metadata": {},
   "source": [
    "# Predicting Pawpularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569168e",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "PetFinder.my is Malaysia’s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n",
    "\n",
    "Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n",
    "\n",
    "In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles.\n",
    "\n",
    "#### Description of data\n",
    "- CSV file with 9912 rows and 14 columns of metadata (no nulls)\n",
    "- Folder with 9912 jpeg files linked to metadata via id in file name\n",
    "\n",
    "#### Notes:\n",
    "- Selection method of data is unclear\n",
    "- Unclear whether photos are profile photos\n",
    "- Pawpularity score is based on webtraffic on pet profile, not based on metadata\n",
    "- Metadata does not include information whether featured pets are dog or cat\n",
    "- No data concerning pet location (which may have significant impact on website traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d30c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/arnet/Desktop/Ironhack/GitHub/Deep_Learning_Final_Project/MetaData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c90307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0f2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Id')\n",
    "data = data.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d88894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0007de18844b0dbbb5e1f607da0606e0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009c66b9439883ba2750fb825e1d7db</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0013fd999caf9a3efe1352ca1b0d937e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0018df346ac9c1d8413cfcc888ca8246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001dc955e10590d3ca4673f034feeef2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0          0      1        0      0          0   \n",
       "0009c66b9439883ba2750fb825e1d7db          0      0        0      0          0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e          0      0        0      1          1   \n",
       "0018df346ac9c1d8413cfcc888ca8246          0      0        0      0          0   \n",
       "001dc955e10590d3ca4673f034feeef2          0      1        0      0          0   \n",
       "\n",
       "                                  Info  Blur  Pawpularity  \n",
       "0007de18844b0dbbb5e1f607da0606e0     0     0           63  \n",
       "0009c66b9439883ba2750fb825e1d7db     0     0           42  \n",
       "0013fd999caf9a3efe1352ca1b0d937e     0     0           28  \n",
       "0018df346ac9c1d8413cfcc888ca8246     0     0           15  \n",
       "001dc955e10590d3ca4673f034feeef2     0     0           72  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6ac667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9912 entries, 0007de18844b0dbbb5e1f607da0606e0 to fff8e47c766799c9e12f3cb3d66ad228\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype\n",
      "---  ------         --------------  -----\n",
      " 0   Subject Focus  9912 non-null   int64\n",
      " 1   Eyes           9912 non-null   int64\n",
      " 2   Face           9912 non-null   int64\n",
      " 3   Near           9912 non-null   int64\n",
      " 4   Action         9912 non-null   int64\n",
      " 5   Accessory      9912 non-null   int64\n",
      " 6   Group          9912 non-null   int64\n",
      " 7   Collage        9912 non-null   int64\n",
      " 8   Human          9912 non-null   int64\n",
      " 9   Occlusion      9912 non-null   int64\n",
      " 10  Info           9912 non-null   int64\n",
      " 11  Blur           9912 non-null   int64\n",
      " 12  Pawpularity    9912 non-null   int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3683bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT FOCUS\n",
      "0    9638\n",
      "1     274\n",
      "Name: Subject Focus, dtype: int64\n",
      "EYES\n",
      "1    7658\n",
      "0    2254\n",
      "Name: Eyes, dtype: int64\n",
      "FACE\n",
      "1    8960\n",
      "0     952\n",
      "Name: Face, dtype: int64\n",
      "NEAR\n",
      "1    8540\n",
      "0    1372\n",
      "Name: Near, dtype: int64\n",
      "ACTION\n",
      "0    9813\n",
      "1      99\n",
      "Name: Action, dtype: int64\n",
      "ACCESSORY\n",
      "0    9240\n",
      "1     672\n",
      "Name: Accessory, dtype: int64\n",
      "GROUP\n",
      "0    8630\n",
      "1    1282\n",
      "Name: Group, dtype: int64\n",
      "COLLAGE\n",
      "0    9420\n",
      "1     492\n",
      "Name: Collage, dtype: int64\n",
      "HUMAN\n",
      "0    8264\n",
      "1    1648\n",
      "Name: Human, dtype: int64\n",
      "OCCLUSION\n",
      "0    8207\n",
      "1    1705\n",
      "Name: Occlusion, dtype: int64\n",
      "INFO\n",
      "0    9305\n",
      "1     607\n",
      "Name: Info, dtype: int64\n",
      "BLUR\n",
      "0    9214\n",
      "1     698\n",
      "Name: Blur, dtype: int64\n",
      "PAWPULARITY\n",
      "28    318\n",
      "30    318\n",
      "26    316\n",
      "31    312\n",
      "29    304\n",
      "     ... \n",
      "98     10\n",
      "97      8\n",
      "90      7\n",
      "1       4\n",
      "99      4\n",
      "Name: Pawpularity, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in data:\n",
    "    print(column.upper())\n",
    "    print(data[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d83fb0",
   "metadata": {},
   "source": [
    "## Process metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cf224",
   "metadata": {},
   "source": [
    "### Scale pawpularity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6d9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target score\n",
    "y = data[\"Pawpularity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796663bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and ensure that score is between 0 and 1\n",
    "maxScore = y.max()\n",
    "y_scaled= y / maxScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3547823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0007de18844b0dbbb5e1f607da0606e0    0.63\n",
       "0009c66b9439883ba2750fb825e1d7db    0.42\n",
       "0013fd999caf9a3efe1352ca1b0d937e    0.28\n",
       "0018df346ac9c1d8413cfcc888ca8246    0.15\n",
       "001dc955e10590d3ca4673f034feeef2    0.72\n",
       "                                    ... \n",
       "ffbfa0383c34dc513c95560d6e1fdb57    0.15\n",
       "ffcc8532d76436fc79e50eb2e5238e45    0.70\n",
       "ffdf2e8673a1da6fb80342fa3b119a20    0.20\n",
       "fff19e2ce11718548fa1c5d039a5192a    0.20\n",
       "fff8e47c766799c9e12f3cb3d66ad228    0.30\n",
       "Name: Pawpularity, Length: 9912, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06dc1290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0007de18844b0dbbb5e1f607da0606e0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009c66b9439883ba2750fb825e1d7db</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0013fd999caf9a3efe1352ca1b0d937e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0018df346ac9c1d8413cfcc888ca8246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001dc955e10590d3ca4673f034feeef2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0          0      1        0      0          0   \n",
       "0009c66b9439883ba2750fb825e1d7db          0      0        0      0          0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e          0      0        0      1          1   \n",
       "0018df346ac9c1d8413cfcc888ca8246          0      0        0      0          0   \n",
       "001dc955e10590d3ca4673f034feeef2          0      1        0      0          0   \n",
       "\n",
       "                                  Info  Blur  \n",
       "0007de18844b0dbbb5e1f607da0606e0     0     0  \n",
       "0009c66b9439883ba2750fb825e1d7db     0     0  \n",
       "0013fd999caf9a3efe1352ca1b0d937e     0     0  \n",
       "0018df346ac9c1d8413cfcc888ca8246     0     0  \n",
       "001dc955e10590d3ca4673f034feeef2     0     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 'y'\n",
    "data = data.drop('Pawpularity', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3465c",
   "metadata": {},
   "source": [
    "## Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f60d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0ebe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_images(data, inputPath):\n",
    "    # initialize images array (i.e., the pet images themselves)\n",
    "    images = []\n",
    "   \n",
    "    # get image files based on id in dataframe index column\n",
    "    for i in data.index.values:\n",
    "        base = os.path.join(inputPath, i).replace('\\\\','/')\n",
    "        basePath = f'{base}.jpg'\n",
    "        image = cv2.imread(basePath)\n",
    "        \n",
    "        # Resize images\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        images.append(image)\n",
    "    \n",
    "    # return set of images\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818bb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide input path for loading images\n",
    "inputPath = '/Users/arnet/Desktop/Ironhack/GitHub/Deep_Learning_Final_Project/Images'\n",
    "\n",
    "# make input path the current directory\n",
    "os.chdir(inputPath)\n",
    "\n",
    "images = load_house_images(data, inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be25305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab0ed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9912"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ad8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale image size\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7342007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.76078431, 0.70588235, 0.72156863],\n",
       "        [0.69803922, 0.64313725, 0.65882353],\n",
       "        [0.78431373, 0.73333333, 0.7372549 ],\n",
       "        ...,\n",
       "        [0.69411765, 0.69019608, 0.69803922],\n",
       "        [0.72941176, 0.73333333, 0.7254902 ],\n",
       "        [0.74901961, 0.74117647, 0.7372549 ]],\n",
       "\n",
       "       [[0.75294118, 0.70588235, 0.69803922],\n",
       "        [0.8       , 0.75294118, 0.74509804],\n",
       "        [0.77647059, 0.72941176, 0.72156863],\n",
       "        ...,\n",
       "        [0.56078431, 0.65882353, 0.6       ],\n",
       "        [0.56470588, 0.62352941, 0.58039216],\n",
       "        [0.59607843, 0.61960784, 0.59607843]],\n",
       "\n",
       "       [[0.7254902 , 0.67843137, 0.67058824],\n",
       "        [0.74509804, 0.69803922, 0.69019608],\n",
       "        [0.7372549 , 0.69019608, 0.68235294],\n",
       "        ...,\n",
       "        [0.57647059, 0.54509804, 0.54509804],\n",
       "        [0.59215686, 0.57647059, 0.58431373],\n",
       "        [0.52941176, 0.54117647, 0.54509804]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.63137255, 0.63529412, 0.61568627],\n",
       "        [0.55686275, 0.61176471, 0.62352941],\n",
       "        [0.41960784, 0.45882353, 0.45490196],\n",
       "        ...,\n",
       "        [0.69019608, 0.71372549, 0.71372549],\n",
       "        [0.68627451, 0.71764706, 0.74117647],\n",
       "        [0.71764706, 0.73333333, 0.76078431]],\n",
       "\n",
       "       [[0.54117647, 0.58823529, 0.58039216],\n",
       "        [0.56078431, 0.63137255, 0.58823529],\n",
       "        [0.49803922, 0.56862745, 0.50196078],\n",
       "        ...,\n",
       "        [0.72156863, 0.72941176, 0.73333333],\n",
       "        [0.70980392, 0.70588235, 0.69803922],\n",
       "        [0.7372549 , 0.74509804, 0.75294118]],\n",
       "\n",
       "       [[0.55686275, 0.57254902, 0.58823529],\n",
       "        [0.72156863, 0.72156863, 0.73333333],\n",
       "        [0.64313725, 0.62745098, 0.62352941],\n",
       "        ...,\n",
       "        [0.61960784, 0.63529412, 0.63921569],\n",
       "        [0.75686275, 0.77254902, 0.77647059],\n",
       "        [0.74509804, 0.76078431, 0.76470588]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95f5e6",
   "metadata": {},
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafb3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf1e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for METADATA, IMAGES AND PREDICTION VALUE 'y'\n",
    "split = train_test_split(data, images, y, test_size=0.25, random_state=42)\n",
    "(X_train, X_test, XImages_train, XImages_test, y_train, y_test) = split\n",
    "\n",
    "# experiments were initially run using 'y-scaled', but using 'y' helps to judge accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b29d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5e1a107f1f714592cd96d3e1329cab27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8d7c28145cfbdd2349acd8f2a28949f</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20ebe45894e740a627da088335f9ddea</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c041fd9ed4c83ac88a3e9e0487a02ec</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803bb1db4f00079a0be91f3319de78f9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "5e1a107f1f714592cd96d3e1329cab27              0     1     1     1       0   \n",
       "e8d7c28145cfbdd2349acd8f2a28949f              0     1     1     0       0   \n",
       "20ebe45894e740a627da088335f9ddea              0     0     0     0       0   \n",
       "7c041fd9ed4c83ac88a3e9e0487a02ec              0     1     1     1       0   \n",
       "803bb1db4f00079a0be91f3319de78f9              0     1     1     1       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "5e1a107f1f714592cd96d3e1329cab27          0      0        0      0          0   \n",
       "e8d7c28145cfbdd2349acd8f2a28949f          0      0        1      0          1   \n",
       "20ebe45894e740a627da088335f9ddea          0      1        0      0          0   \n",
       "7c041fd9ed4c83ac88a3e9e0487a02ec          0      0        0      1          0   \n",
       "803bb1db4f00079a0be91f3319de78f9          0      0        0      0          0   \n",
       "\n",
       "                                  Info  Blur  \n",
       "5e1a107f1f714592cd96d3e1329cab27     0     0  \n",
       "e8d7c28145cfbdd2349acd8f2a28949f     1     0  \n",
       "20ebe45894e740a627da088335f9ddea     0     0  \n",
       "7c041fd9ed4c83ac88a3e9e0487a02ec     0     0  \n",
       "803bb1db4f00079a0be91f3319de78f9     0     0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6ae9c",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52663717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbef979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f7c20",
   "metadata": {},
   "source": [
    "## Simple MLP model for Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2deef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating 3-layer MLP model\n",
    "def create_mlp(dim, regress=False):\n",
    "    # define MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e053c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = create_mlp(X_train.shape[1], regress=True)\n",
    "\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer='adam',\n",
    "              metrics = ['mse', \n",
    "                         'mean_absolute_error', \n",
    "                         tf.metrics.RootMeanSquaredError()] \n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074bad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 1696.1869 - mse: 1696.1869 - mean_absolute_error: 35.7449 - root_mean_squared_error: 41.1848 - val_loss: 1427.7640 - val_mse: 1427.7640 - val_mean_absolute_error: 31.4951 - val_root_mean_squared_error: 37.7858\n",
      "Epoch 2/10\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 908.4112 - mse: 908.4112 - mean_absolute_error: 22.9663 - root_mean_squared_error: 30.1399 - val_loss: 580.7789 - val_mse: 580.7789 - val_mean_absolute_error: 16.7456 - val_root_mean_squared_error: 24.0994\n",
      "Epoch 3/10\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 490.8665 - mse: 490.8665 - mean_absolute_error: 16.0642 - root_mean_squared_error: 22.1555 - val_loss: 509.1236 - val_mse: 509.1236 - val_mean_absolute_error: 16.6564 - val_root_mean_squared_error: 22.5638\n",
      "Epoch 4/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 469.5642 - mse: 469.5642 - mean_absolute_error: 16.1107 - root_mean_squared_error: 21.6694 - val_loss: 500.1514 - val_mse: 500.1514 - val_mean_absolute_error: 16.6342 - val_root_mean_squared_error: 22.3641\n",
      "Epoch 5/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 460.5203 - mse: 460.5203 - mean_absolute_error: 16.0167 - root_mean_squared_error: 21.4597 - val_loss: 490.2987 - val_mse: 490.2987 - val_mean_absolute_error: 16.3839 - val_root_mean_squared_error: 22.1427\n",
      "Epoch 6/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 451.7854 - mse: 451.7854 - mean_absolute_error: 15.8309 - root_mean_squared_error: 21.2552 - val_loss: 482.3360 - val_mse: 482.3360 - val_mean_absolute_error: 16.2970 - val_root_mean_squared_error: 21.9621\n",
      "Epoch 7/10\n",
      "233/233 [==============================] - 1s 2ms/step - loss: 445.2441 - mse: 445.2441 - mean_absolute_error: 15.7014 - root_mean_squared_error: 21.1008 - val_loss: 476.4701 - val_mse: 476.4701 - val_mean_absolute_error: 16.2928 - val_root_mean_squared_error: 21.8282\n",
      "Epoch 8/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 440.4831 - mse: 440.4831 - mean_absolute_error: 15.6625 - root_mean_squared_error: 20.9877 - val_loss: 471.4015 - val_mse: 471.4015 - val_mean_absolute_error: 16.1823 - val_root_mean_squared_error: 21.7118\n",
      "Epoch 9/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 436.7560 - mse: 436.7560 - mean_absolute_error: 15.5745 - root_mean_squared_error: 20.8987 - val_loss: 467.6888 - val_mse: 467.6888 - val_mean_absolute_error: 16.1640 - val_root_mean_squared_error: 21.6261\n",
      "Epoch 10/10\n",
      "233/233 [==============================] - 0s 2ms/step - loss: 433.8870 - mse: 433.8870 - mean_absolute_error: 15.5594 - root_mean_squared_error: 20.8300 - val_loss: 464.4777 - val_mse: 464.4777 - val_mean_absolute_error: 15.9245 - val_root_mean_squared_error: 21.5517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21db52330d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(x=X_train, y=y_train, \n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb623aa",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Working without batches produced slightly better scores\n",
    "* Number of epochs was reduced from an initial number of 100\n",
    "* Working with y-scaled resulted in approximately equal results in terms of percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e7524",
   "metadata": {},
   "source": [
    "## Single-layer CNN model for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53beba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# One convolutional layer followed by max-pooling, flattening and dense layers\n",
    "model2 = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(25),\n",
    "    Activation('relu')\n",
    "])\n",
    "\n",
    "model2.compile(loss=\"mse\",\n",
    "              optimizer='adam',\n",
    "              metrics=['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44351288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "930/930 [==============================] - 41s 44ms/step - loss: 469.8501 - mse: 469.8501 - root_mean_squared_error: 21.6760 - mean_absolute_error: 16.1467 - val_loss: 458.3836 - val_mse: 458.3836 - val_root_mean_squared_error: 21.4099 - val_mean_absolute_error: 16.0968\n",
      "Epoch 2/5\n",
      "930/930 [==============================] - 47s 50ms/step - loss: 435.8248 - mse: 435.8248 - root_mean_squared_error: 20.8764 - mean_absolute_error: 15.6268 - val_loss: 452.5438 - val_mse: 452.5438 - val_root_mean_squared_error: 21.2731 - val_mean_absolute_error: 15.8140\n",
      "Epoch 3/5\n",
      "930/930 [==============================] - 48s 52ms/step - loss: 420.3949 - mse: 420.3949 - root_mean_squared_error: 20.5035 - mean_absolute_error: 15.3755 - val_loss: 487.2935 - val_mse: 487.2935 - val_root_mean_squared_error: 22.0747 - val_mean_absolute_error: 15.3746\n",
      "Epoch 4/5\n",
      "930/930 [==============================] - 46s 49ms/step - loss: 405.0225 - mse: 405.0225 - root_mean_squared_error: 20.1252 - mean_absolute_error: 15.0819 - val_loss: 454.1357 - val_mse: 454.1357 - val_root_mean_squared_error: 21.3105 - val_mean_absolute_error: 16.3083\n",
      "Epoch 5/5\n",
      "930/930 [==============================] - 53s 57ms/step - loss: 392.8087 - mse: 392.8087 - root_mean_squared_error: 19.8194 - mean_absolute_error: 14.8923 - val_loss: 469.2653 - val_mse: 469.2653 - val_root_mean_squared_error: 21.6625 - val_mean_absolute_error: 17.0122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21db559e490>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model2.fit(x=XImages_train, y=y_train,\n",
    "           validation_data=(XImages_test, y_test),\n",
    "           epochs=5, \n",
    "           batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0477f0",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Tests were run with different numbers of neurons in Dense layer, 25 provided the best results\n",
    "* Signs of overfitting nonetheless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d658",
   "metadata": {},
   "source": [
    "## Double-layer CNN model for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfeca557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(32, activation= 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(8, activation= 'relu'),\n",
    "])\n",
    "\n",
    "model3.compile(loss=\"mse\",\n",
    "              optimizer='adam', \n",
    "              metrics=['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d829d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "930/930 [==============================] - 93s 100ms/step - loss: 1331.3000 - mse: 1331.3000 - root_mean_squared_error: 36.4870 - mean_absolute_error: 29.6737 - val_loss: 876.1465 - val_mse: 876.1465 - val_root_mean_squared_error: 29.5998 - val_mean_absolute_error: 22.0638\n",
      "Epoch 2/5\n",
      "930/930 [==============================] - 101s 108ms/step - loss: 475.2072 - mse: 475.2072 - root_mean_squared_error: 21.7992 - mean_absolute_error: 15.9092 - val_loss: 468.4354 - val_mse: 468.4354 - val_root_mean_squared_error: 21.6434 - val_mean_absolute_error: 17.0523\n",
      "Epoch 3/5\n",
      "930/930 [==============================] - 120s 130ms/step - loss: 408.5860 - mse: 408.5860 - root_mean_squared_error: 20.2135 - mean_absolute_error: 15.1692 - val_loss: 462.0864 - val_mse: 462.0864 - val_root_mean_squared_error: 21.4962 - val_mean_absolute_error: 16.1954\n",
      "Epoch 4/5\n",
      "930/930 [==============================] - 132s 141ms/step - loss: 386.5220 - mse: 386.5220 - root_mean_squared_error: 19.6602 - mean_absolute_error: 14.8684 - val_loss: 486.7852 - val_mse: 486.7852 - val_root_mean_squared_error: 22.0632 - val_mean_absolute_error: 16.9063\n",
      "Epoch 5/5\n",
      "930/930 [==============================] - 164s 176ms/step - loss: 350.0587 - mse: 350.0587 - root_mean_squared_error: 18.7099 - mean_absolute_error: 14.1571 - val_loss: 589.4374 - val_mse: 589.4374 - val_root_mean_squared_error: 24.2783 - val_mean_absolute_error: 18.3073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21db5a73d00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model3.fit(x=XImages_train, y=y_train,\n",
    "           validation_data=(XImages_test, y_test),\n",
    "           epochs=5, \n",
    "           batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011ba73",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Adding layers for batch normalization significantly accelerates training rate, nonetheless uch slower than single-layer model\n",
    "* Once again, tests were done with different numbers of neurons in the layers (ex: 32 and 16, 20 and 10) and also adding a third layer. The configuration of 32 and 8 provides reasonable results in terms of speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73faacb",
   "metadata": {},
   "source": [
    "## Combined Model - Images and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64362035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for regression prediction FOR IMAGE DATA\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64)):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1  # indicates axis which should be normalised, \n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    \n",
    "    # loop over the three filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # flatten, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "949e155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE the MLP and CNN models\n",
    "mlp = create_mlp(X_train.shape[1], regress=False) # X_train.shape[1] = number of METADATA columns i.e. inputs\n",
    "cnn = create_cnn(64, 64, 3)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# final model accepts categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting the Pawpularity score\n",
    "model4 = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f15d3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "930/930 [==============================] - 129s 139ms/step - loss: 703.6846 - mse: 703.6846 - root_mean_squared_error: 26.5271 - mean_absolute_error: 19.8066 - val_loss: 479.0067 - val_mse: 479.0067 - val_root_mean_squared_error: 21.8862 - val_mean_absolute_error: 16.8611\n",
      "Epoch 2/5\n",
      "930/930 [==============================] - 144s 155ms/step - loss: 450.1529 - mse: 450.1529 - root_mean_squared_error: 21.2168 - mean_absolute_error: 15.8090 - val_loss: 471.5981 - val_mse: 471.5981 - val_root_mean_squared_error: 21.7163 - val_mean_absolute_error: 16.2659\n",
      "Epoch 3/5\n",
      "930/930 [==============================] - 144s 155ms/step - loss: 436.3937 - mse: 436.3937 - root_mean_squared_error: 20.8900 - mean_absolute_error: 15.5927 - val_loss: 461.4569 - val_mse: 461.4569 - val_root_mean_squared_error: 21.4815 - val_mean_absolute_error: 15.6589\n",
      "Epoch 4/5\n",
      "930/930 [==============================] - 123s 133ms/step - loss: 429.5535 - mse: 429.5535 - root_mean_squared_error: 20.7257 - mean_absolute_error: 15.4385 - val_loss: 454.2485 - val_mse: 454.2485 - val_root_mean_squared_error: 21.3131 - val_mean_absolute_error: 15.7412\n",
      "Epoch 5/5\n",
      "930/930 [==============================] - 145s 156ms/step - loss: 422.9255 - mse: 422.9255 - root_mean_squared_error: 20.5652 - mean_absolute_error: 15.3794 - val_loss: 450.8160 - val_mse: 450.8160 - val_root_mean_squared_error: 21.2324 - val_mean_absolute_error: 15.6165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21db6c02c70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model4.compile(loss=\"mse\", \n",
    "              metrics = ['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'],\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "# train the model\n",
    "model4.fit(x=[X_train, XImages_train], y=y_train, \n",
    "          validation_data=([X_test, XImages_test], y_test), \n",
    "          epochs=5, \n",
    "          batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c1bf7",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* By far the slowest model\n",
    "* No improvements regarding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574936f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
