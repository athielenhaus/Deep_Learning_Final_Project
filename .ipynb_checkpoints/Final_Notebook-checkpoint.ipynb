{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde8f59f",
   "metadata": {},
   "source": [
    "# Predicting Pawpularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569168e",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "PetFinder.my is Malaysia’s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n",
    "\n",
    "Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n",
    "\n",
    "In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles.\n",
    "\n",
    "#### Description of data\n",
    "- CSV file with 9912 rows and 14 columns of metadata (no nulls)\n",
    "- Folder with 9912 jpeg files linked to metadata via id in file name\n",
    "\n",
    "#### Notes:\n",
    "- Selection method of data is unclear\n",
    "- Unclear whether photos are profile photos\n",
    "- Pawpularity score is based on webtraffic on pet profile, not based on metadata\n",
    "- Metadata does not include information whether featured pets are dog or cat\n",
    "- No data concerning pet location (which may have significant impact on website traffic)\n",
    "\n",
    "#### Procedure:\n",
    "Test 4 different deep learning models, including:\n",
    "* Multilayer Perceptron (MLP) Model for only the metadata\n",
    "* two different Convoluted Neural Networks (CNNs) with a varying number of dense layers and only the images as input\n",
    "* one mixed model using both the metadata and images as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec59d1",
   "metadata": {},
   "source": [
    "## Load and review Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d30c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/arnet/Desktop/Ironhack/GitHub/Deep_Learning_Final_Project/MetaData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c90307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0f2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Id')\n",
    "data = data.rename_axis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d88894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0007de18844b0dbbb5e1f607da0606e0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009c66b9439883ba2750fb825e1d7db</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0013fd999caf9a3efe1352ca1b0d937e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0018df346ac9c1d8413cfcc888ca8246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001dc955e10590d3ca4673f034feeef2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0          0      1        0      0          0   \n",
       "0009c66b9439883ba2750fb825e1d7db          0      0        0      0          0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e          0      0        0      1          1   \n",
       "0018df346ac9c1d8413cfcc888ca8246          0      0        0      0          0   \n",
       "001dc955e10590d3ca4673f034feeef2          0      1        0      0          0   \n",
       "\n",
       "                                  Info  Blur  Pawpularity  \n",
       "0007de18844b0dbbb5e1f607da0606e0     0     0           63  \n",
       "0009c66b9439883ba2750fb825e1d7db     0     0           42  \n",
       "0013fd999caf9a3efe1352ca1b0d937e     0     0           28  \n",
       "0018df346ac9c1d8413cfcc888ca8246     0     0           15  \n",
       "001dc955e10590d3ca4673f034feeef2     0     0           72  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6ac667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9912 entries, 0007de18844b0dbbb5e1f607da0606e0 to fff8e47c766799c9e12f3cb3d66ad228\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype\n",
      "---  ------         --------------  -----\n",
      " 0   Subject Focus  9912 non-null   int64\n",
      " 1   Eyes           9912 non-null   int64\n",
      " 2   Face           9912 non-null   int64\n",
      " 3   Near           9912 non-null   int64\n",
      " 4   Action         9912 non-null   int64\n",
      " 5   Accessory      9912 non-null   int64\n",
      " 6   Group          9912 non-null   int64\n",
      " 7   Collage        9912 non-null   int64\n",
      " 8   Human          9912 non-null   int64\n",
      " 9   Occlusion      9912 non-null   int64\n",
      " 10  Info           9912 non-null   int64\n",
      " 11  Blur           9912 non-null   int64\n",
      " 12  Pawpularity    9912 non-null   int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3683bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT FOCUS\n",
      "0    9638\n",
      "1     274\n",
      "Name: Subject Focus, dtype: int64\n",
      "EYES\n",
      "1    7658\n",
      "0    2254\n",
      "Name: Eyes, dtype: int64\n",
      "FACE\n",
      "1    8960\n",
      "0     952\n",
      "Name: Face, dtype: int64\n",
      "NEAR\n",
      "1    8540\n",
      "0    1372\n",
      "Name: Near, dtype: int64\n",
      "ACTION\n",
      "0    9813\n",
      "1      99\n",
      "Name: Action, dtype: int64\n",
      "ACCESSORY\n",
      "0    9240\n",
      "1     672\n",
      "Name: Accessory, dtype: int64\n",
      "GROUP\n",
      "0    8630\n",
      "1    1282\n",
      "Name: Group, dtype: int64\n",
      "COLLAGE\n",
      "0    9420\n",
      "1     492\n",
      "Name: Collage, dtype: int64\n",
      "HUMAN\n",
      "0    8264\n",
      "1    1648\n",
      "Name: Human, dtype: int64\n",
      "OCCLUSION\n",
      "0    8207\n",
      "1    1705\n",
      "Name: Occlusion, dtype: int64\n",
      "INFO\n",
      "0    9305\n",
      "1     607\n",
      "Name: Info, dtype: int64\n",
      "BLUR\n",
      "0    9214\n",
      "1     698\n",
      "Name: Blur, dtype: int64\n",
      "PAWPULARITY\n",
      "28    318\n",
      "30    318\n",
      "26    316\n",
      "31    312\n",
      "29    304\n",
      "     ... \n",
      "98     10\n",
      "97      8\n",
      "90      7\n",
      "1       4\n",
      "99      4\n",
      "Name: Pawpularity, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for column in data:\n",
    "    print(column.upper())\n",
    "    print(data[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d83fb0",
   "metadata": {},
   "source": [
    "## Process metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cf224",
   "metadata": {},
   "source": [
    "### Scale pawpularity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6d9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target score\n",
    "y = data[\"Pawpularity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796663bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and ensure that score is between 0 and 1\n",
    "maxScore = y.max()\n",
    "y_scaled= y / maxScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3547823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0007de18844b0dbbb5e1f607da0606e0    0.63\n",
       "0009c66b9439883ba2750fb825e1d7db    0.42\n",
       "0013fd999caf9a3efe1352ca1b0d937e    0.28\n",
       "0018df346ac9c1d8413cfcc888ca8246    0.15\n",
       "001dc955e10590d3ca4673f034feeef2    0.72\n",
       "                                    ... \n",
       "ffbfa0383c34dc513c95560d6e1fdb57    0.15\n",
       "ffcc8532d76436fc79e50eb2e5238e45    0.70\n",
       "ffdf2e8673a1da6fb80342fa3b119a20    0.20\n",
       "fff19e2ce11718548fa1c5d039a5192a    0.20\n",
       "fff8e47c766799c9e12f3cb3d66ad228    0.30\n",
       "Name: Pawpularity, Length: 9912, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06dc1290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0007de18844b0dbbb5e1f607da0606e0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0009c66b9439883ba2750fb825e1d7db</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0013fd999caf9a3efe1352ca1b0d937e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0018df346ac9c1d8413cfcc888ca8246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001dc955e10590d3ca4673f034feeef2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "0007de18844b0dbbb5e1f607da0606e0          0      1        0      0          0   \n",
       "0009c66b9439883ba2750fb825e1d7db          0      0        0      0          0   \n",
       "0013fd999caf9a3efe1352ca1b0d937e          0      0        0      1          1   \n",
       "0018df346ac9c1d8413cfcc888ca8246          0      0        0      0          0   \n",
       "001dc955e10590d3ca4673f034feeef2          0      1        0      0          0   \n",
       "\n",
       "                                  Info  Blur  \n",
       "0007de18844b0dbbb5e1f607da0606e0     0     0  \n",
       "0009c66b9439883ba2750fb825e1d7db     0     0  \n",
       "0013fd999caf9a3efe1352ca1b0d937e     0     0  \n",
       "0018df346ac9c1d8413cfcc888ca8246     0     0  \n",
       "001dc955e10590d3ca4673f034feeef2     0     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 'y'\n",
    "data = data.drop('Pawpularity', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3465c",
   "metadata": {},
   "source": [
    "## Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f60d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0ebe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pet_images(data, inputPath):\n",
    "    # initialize images array (i.e., the pet images themselves)\n",
    "    images = []\n",
    "   \n",
    "    # get image files based on id in dataframe index column\n",
    "    for i in data.index.values:\n",
    "        base = os.path.join(inputPath, i).replace('\\\\','/')\n",
    "        basePath = f'{base}.jpg'\n",
    "        image = cv2.imread(basePath)\n",
    "        \n",
    "        # Resize images\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        images.append(image)\n",
    "    \n",
    "    # return set of images\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818bb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide input path for loading images\n",
    "inputPath = '/Users/arnet/Desktop/Ironhack/GitHub/Deep_Learning_Final_Project/Images'\n",
    "\n",
    "# make input path the current directory\n",
    "os.chdir(inputPath)\n",
    "\n",
    "images = load_pet_images(data, inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3be25305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ab0ed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9912"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ad8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale image size\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7342007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.76078431, 0.70588235, 0.72156863],\n",
       "        [0.69803922, 0.64313725, 0.65882353],\n",
       "        [0.78431373, 0.73333333, 0.7372549 ],\n",
       "        ...,\n",
       "        [0.69411765, 0.69019608, 0.69803922],\n",
       "        [0.72941176, 0.73333333, 0.7254902 ],\n",
       "        [0.74901961, 0.74117647, 0.7372549 ]],\n",
       "\n",
       "       [[0.75294118, 0.70588235, 0.69803922],\n",
       "        [0.8       , 0.75294118, 0.74509804],\n",
       "        [0.77647059, 0.72941176, 0.72156863],\n",
       "        ...,\n",
       "        [0.56078431, 0.65882353, 0.6       ],\n",
       "        [0.56470588, 0.62352941, 0.58039216],\n",
       "        [0.59607843, 0.61960784, 0.59607843]],\n",
       "\n",
       "       [[0.7254902 , 0.67843137, 0.67058824],\n",
       "        [0.74509804, 0.69803922, 0.69019608],\n",
       "        [0.7372549 , 0.69019608, 0.68235294],\n",
       "        ...,\n",
       "        [0.57647059, 0.54509804, 0.54509804],\n",
       "        [0.59215686, 0.57647059, 0.58431373],\n",
       "        [0.52941176, 0.54117647, 0.54509804]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.63137255, 0.63529412, 0.61568627],\n",
       "        [0.55686275, 0.61176471, 0.62352941],\n",
       "        [0.41960784, 0.45882353, 0.45490196],\n",
       "        ...,\n",
       "        [0.69019608, 0.71372549, 0.71372549],\n",
       "        [0.68627451, 0.71764706, 0.74117647],\n",
       "        [0.71764706, 0.73333333, 0.76078431]],\n",
       "\n",
       "       [[0.54117647, 0.58823529, 0.58039216],\n",
       "        [0.56078431, 0.63137255, 0.58823529],\n",
       "        [0.49803922, 0.56862745, 0.50196078],\n",
       "        ...,\n",
       "        [0.72156863, 0.72941176, 0.73333333],\n",
       "        [0.70980392, 0.70588235, 0.69803922],\n",
       "        [0.7372549 , 0.74509804, 0.75294118]],\n",
       "\n",
       "       [[0.55686275, 0.57254902, 0.58823529],\n",
       "        [0.72156863, 0.72156863, 0.73333333],\n",
       "        [0.64313725, 0.62745098, 0.62352941],\n",
       "        ...,\n",
       "        [0.61960784, 0.63529412, 0.63921569],\n",
       "        [0.75686275, 0.77254902, 0.77647059],\n",
       "        [0.74509804, 0.76078431, 0.76470588]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95f5e6",
   "metadata": {},
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bafb3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbf1e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for METADATA, IMAGES AND PREDICTION VALUE 'y'\n",
    "split = train_test_split(data, images, y, test_size=0.20, random_state=42)\n",
    "(X_train_full, X_test, XImages_train_full, XImages_test, y_train_full, y_test) = split\n",
    "\n",
    "# experiments were initially run using 'y-scaled', but using 'y' helps to judge accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb4813ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set AGAIN to obtain VALIDATION SET\n",
    "split_val = train_test_split(X_train_full, XImages_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "(X_train, X_val, XImages_train, XImages_val, y_train, y_val) = split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b12f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6343\n",
      "1586\n",
      "6343\n",
      "1586\n",
      "6343\n",
      "1586\n"
     ]
    }
   ],
   "source": [
    "for x in split_val:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b29d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e887fce17c574c0d0b24a611d4c17c9e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4b249023b0234766daa3258ac27577ac</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e761f47cc1e3038a431f9f196234ab9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5c736b9e97fce993dc1c8b9f451618cb</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80764b6de6f6554b8ac0363e88b4d064</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "e887fce17c574c0d0b24a611d4c17c9e              0     0     1     1       0   \n",
       "4b249023b0234766daa3258ac27577ac              0     1     1     1       0   \n",
       "7e761f47cc1e3038a431f9f196234ab9              0     1     1     1       0   \n",
       "5c736b9e97fce993dc1c8b9f451618cb              0     1     1     1       0   \n",
       "80764b6de6f6554b8ac0363e88b4d064              0     1     1     0       0   \n",
       "\n",
       "                                  Accessory  Group  Collage  Human  Occlusion  \\\n",
       "e887fce17c574c0d0b24a611d4c17c9e          0      1        0      0          0   \n",
       "4b249023b0234766daa3258ac27577ac          0      0        0      0          0   \n",
       "7e761f47cc1e3038a431f9f196234ab9          0      0        0      0          1   \n",
       "5c736b9e97fce993dc1c8b9f451618cb          0      0        0      1          1   \n",
       "80764b6de6f6554b8ac0363e88b4d064          0      0        0      0          1   \n",
       "\n",
       "                                  Info  Blur  \n",
       "e887fce17c574c0d0b24a611d4c17c9e     0     0  \n",
       "4b249023b0234766daa3258ac27577ac     0     0  \n",
       "7e761f47cc1e3038a431f9f196234ab9     0     0  \n",
       "5c736b9e97fce993dc1c8b9f451618cb     0     0  \n",
       "80764b6de6f6554b8ac0363e88b4d064     1     0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6ae9c",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52663717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbef979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f7c20",
   "metadata": {},
   "source": [
    "## Simple MLP model for Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2deef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating 3-layer MLP model\n",
    "def create_mlp(dim, regress=False):\n",
    "    # define MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e053c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = create_mlp(X_train.shape[1], regress=True)\n",
    "\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer='adam',\n",
    "              metrics = ['mse', \n",
    "                         'mean_absolute_error', \n",
    "                         tf.metrics.RootMeanSquaredError()] \n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "074bad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 1693.6410 - mse: 1693.6410 - mean_absolute_error: 35.7796 - root_mean_squared_error: 41.1539 - val_loss: 1492.9996 - val_mse: 1492.9996 - val_mean_absolute_error: 32.3687 - val_root_mean_squared_error: 38.6394\n",
      "Epoch 2/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 886.2848 - mse: 886.2848 - mean_absolute_error: 22.6243 - root_mean_squared_error: 29.7705 - val_loss: 574.3020 - val_mse: 574.3020 - val_mean_absolute_error: 16.6760 - val_root_mean_squared_error: 23.9646\n",
      "Epoch 3/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 473.0845 - mse: 473.0845 - mean_absolute_error: 15.9175 - root_mean_squared_error: 21.7505 - val_loss: 509.3859 - val_mse: 509.3859 - val_mean_absolute_error: 16.5981 - val_root_mean_squared_error: 22.5696\n",
      "Epoch 4/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 457.2582 - mse: 457.2582 - mean_absolute_error: 15.8917 - root_mean_squared_error: 21.3836 - val_loss: 501.1556 - val_mse: 501.1556 - val_mean_absolute_error: 16.5563 - val_root_mean_squared_error: 22.3865\n",
      "Epoch 5/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 451.7163 - mse: 451.7163 - mean_absolute_error: 15.8477 - root_mean_squared_error: 21.2536 - val_loss: 495.9865 - val_mse: 495.9865 - val_mean_absolute_error: 16.5121 - val_root_mean_squared_error: 22.2708\n",
      "Epoch 6/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 447.7542 - mse: 447.7542 - mean_absolute_error: 15.8186 - root_mean_squared_error: 21.1602 - val_loss: 492.6426 - val_mse: 492.6426 - val_mean_absolute_error: 16.4096 - val_root_mean_squared_error: 22.1956\n",
      "Epoch 7/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 444.7911 - mse: 444.7911 - mean_absolute_error: 15.7708 - root_mean_squared_error: 21.0901 - val_loss: 490.7432 - val_mse: 490.7432 - val_mean_absolute_error: 16.2808 - val_root_mean_squared_error: 22.1527\n",
      "Epoch 8/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 442.1031 - mse: 442.1031 - mean_absolute_error: 15.6640 - root_mean_squared_error: 21.0262 - val_loss: 486.1742 - val_mse: 486.1742 - val_mean_absolute_error: 16.3896 - val_root_mean_squared_error: 22.0494\n",
      "Epoch 9/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 439.6789 - mse: 439.6789 - mean_absolute_error: 15.6747 - root_mean_squared_error: 20.9685 - val_loss: 484.3354 - val_mse: 484.3354 - val_mean_absolute_error: 16.2542 - val_root_mean_squared_error: 22.0076\n",
      "Epoch 10/10\n",
      "199/199 [==============================] - 0s 2ms/step - loss: 437.1968 - mse: 437.1968 - mean_absolute_error: 15.6148 - root_mean_squared_error: 20.9093 - val_loss: 482.1198 - val_mse: 482.1198 - val_mean_absolute_error: 16.1795 - val_root_mean_squared_error: 21.9572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d20137d00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(x=X_train, y=y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3831c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on TEST DATA\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d81cfa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  16.109952986270446\n"
     ]
    }
   ],
   "source": [
    "# Get difference\n",
    "diff = preds.flatten() - y_test\n",
    "\n",
    "abs_error = np.abs(diff)\n",
    "mean_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mean_abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb623aa",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Working without batches produced slightly better scores\n",
    "* Number of epochs was reduced from an initial number of 100\n",
    "* Working with y-scaled resulted in approximately equal results in terms of percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e7524",
   "metadata": {},
   "source": [
    "## Single-layer CNN model for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53beba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "# One convolutional layer followed by max-pooling, flattening and dense layers\n",
    "model2 = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(1),           \n",
    "    Activation('linear')    \n",
    "])\n",
    "\n",
    "model2.compile(loss=\"mse\",\n",
    "              optimizer='adam',\n",
    "              metrics=['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44351288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "793/793 [==============================] - 30s 38ms/step - loss: 456.9445 - mse: 456.9445 - root_mean_squared_error: 21.3763 - mean_absolute_error: 15.9552 - val_loss: 464.7738 - val_mse: 464.7738 - val_root_mean_squared_error: 21.5586 - val_mean_absolute_error: 15.7768\n",
      "Epoch 2/5\n",
      "793/793 [==============================] - 35s 44ms/step - loss: 429.7616 - mse: 429.7616 - root_mean_squared_error: 20.7307 - mean_absolute_error: 15.5087 - val_loss: 486.3787 - val_mse: 486.3787 - val_root_mean_squared_error: 22.0540 - val_mean_absolute_error: 15.2800\n",
      "Epoch 3/5\n",
      "793/793 [==============================] - 37s 47ms/step - loss: 418.5968 - mse: 418.5968 - root_mean_squared_error: 20.4596 - mean_absolute_error: 15.3373 - val_loss: 455.1373 - val_mse: 455.1373 - val_root_mean_squared_error: 21.3339 - val_mean_absolute_error: 15.6667\n",
      "Epoch 4/5\n",
      "793/793 [==============================] - 41s 52ms/step - loss: 405.1277 - mse: 405.1277 - root_mean_squared_error: 20.1278 - mean_absolute_error: 15.1245 - val_loss: 458.2195 - val_mse: 458.2195 - val_root_mean_squared_error: 21.4061 - val_mean_absolute_error: 15.7391\n",
      "Epoch 5/5\n",
      "793/793 [==============================] - 47s 59ms/step - loss: 393.8157 - mse: 393.8157 - root_mean_squared_error: 19.8448 - mean_absolute_error: 14.8947 - val_loss: 459.9928 - val_mse: 459.9928 - val_root_mean_squared_error: 21.4474 - val_mean_absolute_error: 15.8260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d20203670>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model2.fit(x=XImages_train, y=y_train,\n",
    "           validation_data=(XImages_val, y_val),\n",
    "           epochs=5, \n",
    "           batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d432d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on TEST DATA\n",
    "preds2 = model2.predict(XImages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "39126cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  15.940919692385275\n"
     ]
    }
   ],
   "source": [
    "diff2 = preds2.flatten() - y_test\n",
    "abs_error = np.abs(diff2)\n",
    "mean_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mean_abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0477f0",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Tests were run with different numbers of neurons in Dense layer, 25 provided the best results\n",
    "* Signs of overfitting nonetheless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d658",
   "metadata": {},
   "source": [
    "## Triple-layer CNN model for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dfeca557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)),\n",
    "    Activation('relu'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(32, activation= 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(8, activation= 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1),           \n",
    "    Activation('linear') \n",
    "])\n",
    "\n",
    "model3.compile(loss=\"mse\",\n",
    "              optimizer='adam', \n",
    "              metrics=['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d829d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "793/793 [==============================] - 76s 95ms/step - loss: 1592.1855 - mse: 1592.1855 - root_mean_squared_error: 39.9022 - mean_absolute_error: 34.2915 - val_loss: 1248.8743 - val_mse: 1248.8743 - val_root_mean_squared_error: 35.3394 - val_mean_absolute_error: 28.61410099 - mean_\n",
      "Epoch 2/5\n",
      "793/793 [==============================] - 87s 110ms/step - loss: 758.7802 - mse: 758.7802 - root_mean_squared_error: 27.5460 - mean_absolute_error: 20.3052 - val_loss: 561.1234 - val_mse: 561.1234 - val_root_mean_squared_error: 23.6880 - val_mean_absolute_error: 16.1356\n",
      "Epoch 3/5\n",
      "793/793 [==============================] - 88s 111ms/step - loss: 426.6953 - mse: 426.6953 - root_mean_squared_error: 20.6566 - mean_absolute_error: 14.8674 - val_loss: 473.0963 - val_mse: 473.0963 - val_root_mean_squared_error: 21.7508 - val_mean_absolute_error: 16.2231\n",
      "Epoch 4/5\n",
      "793/793 [==============================] - 93s 117ms/step - loss: 399.0784 - mse: 399.0784 - root_mean_squared_error: 19.9769 - mean_absolute_error: 14.9639 - val_loss: 469.7734 - val_mse: 469.7734 - val_root_mean_squared_error: 21.6743 - val_mean_absolute_error: 15.6587\n",
      "Epoch 5/5\n",
      "793/793 [==============================] - 99s 124ms/step - loss: 382.7375 - mse: 382.7375 - root_mean_squared_error: 19.5637 - mean_absolute_error: 14.7416 - val_loss: 482.4547 - val_mse: 482.4547 - val_root_mean_squared_error: 21.9649 - val_mean_absolute_error: 15.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d1fc1d700>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model3.fit(x=XImages_train, y=y_train,\n",
    "           validation_data=(XImages_val, y_val),\n",
    "           epochs=5, \n",
    "           batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "57946824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on TEST DATA\n",
    "preds3 = model3.predict(XImages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b42dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  16.102777896117395\n"
     ]
    }
   ],
   "source": [
    "diff3 = preds3.flatten() - y_test\n",
    "abs_error = np.abs(diff3)\n",
    "mean_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mean_abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011ba73",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Adding layers for batch normalization significantly accelerates training rate, nonetheless much slower than single-layer model\n",
    "* Once again, tests were done with different numbers of neurons in the first 2 dense layers (ex: 32 and 16, 20 and 10). The configuration of 32 and 8 provides reasonable results in terms of speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73faacb",
   "metadata": {},
   "source": [
    "## Combined Model - Images and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64362035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for regression prediction FOR IMAGE DATA\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64)):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1  # indicates axis which should be normalised, \n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    \n",
    "    # loop over the three filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # flatten, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "949e155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE the MLP and CNN models\n",
    "mlp = create_mlp(X_train.shape[1], regress=False) # X_train.shape[1] = number of METADATA columns i.e. inputs\n",
    "cnn = create_cnn(64, 64, 3)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# final model accepts categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting the Pawpularity score\n",
    "model4 = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f15d3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "793/793 [==============================] - 169s 213ms/step - loss: 934.3663 - mse: 934.3663 - root_mean_squared_error: 30.5674 - mean_absolute_error: 23.5827 - val_loss: 484.3832 - val_mse: 484.3832 - val_root_mean_squared_error: 22.0087 - val_mean_absolute_error: 16.1153\n",
      "Epoch 2/5\n",
      "793/793 [==============================] - 173s 218ms/step - loss: 449.1079 - mse: 449.1079 - root_mean_squared_error: 21.1922 - mean_absolute_error: 15.8479 - val_loss: 471.5945 - val_mse: 471.5945 - val_root_mean_squared_error: 21.7162 - val_mean_absolute_error: 16.1261\n",
      "Epoch 3/5\n",
      "793/793 [==============================] - 122s 154ms/step - loss: 431.5261 - mse: 431.5261 - root_mean_squared_error: 20.7732 - mean_absolute_error: 15.5144 - val_loss: 470.6503 - val_mse: 470.6503 - val_root_mean_squared_error: 21.6945 - val_mean_absolute_error: 16.0472\n",
      "Epoch 4/5\n",
      "793/793 [==============================] - 143s 180ms/step - loss: 427.1568 - mse: 427.1568 - root_mean_squared_error: 20.6678 - mean_absolute_error: 15.4794 - val_loss: 472.3834 - val_mse: 472.3834 - val_root_mean_squared_error: 21.7344 - val_mean_absolute_error: 15.6252\n",
      "Epoch 5/5\n",
      "793/793 [==============================] - 121s 153ms/step - loss: 422.8732 - mse: 422.8732 - root_mean_squared_error: 20.5639 - mean_absolute_error: 15.3971 - val_loss: 463.3110 - val_mse: 463.3110 - val_root_mean_squared_error: 21.5247 - val_mean_absolute_error: 15.7942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d20ba90a0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model4.compile(loss=\"mse\", \n",
    "              metrics = ['mse', \n",
    "                      tf.keras.metrics.RootMeanSquaredError(),\n",
    "                      'mean_absolute_error'],\n",
    "              optimizer='adam')\n",
    "\n",
    "\n",
    "# train the model\n",
    "model4.fit(x=[X_train, XImages_train], y=y_train, \n",
    "          validation_data=([X_val, XImages_val], y_val), \n",
    "          epochs=5, \n",
    "          batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5727c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on TEST DATA\n",
    "preds4 = model4.predict([X_test, XImages_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ed3435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  15.796010349714209\n"
     ]
    }
   ],
   "source": [
    "diff4 = preds4.flatten() - y_test\n",
    "abs_error = np.abs(diff4)\n",
    "mean_abs_error = np.mean(abs_error)\n",
    "std_abs_error = np.std(abs_error)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mean_abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c1bf7",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* By far the slowest model\n",
    "* No improvements regarding overfitting on validation set\n",
    "* best score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574936f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
